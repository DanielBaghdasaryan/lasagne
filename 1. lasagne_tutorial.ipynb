{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lasagne\n",
    "* lasagne is a library for neural network building and training\n",
    "* it's a low-level library with almost seamless integration with theano\n",
    "\n",
    "For a demo we shall solve the same digit recognition problem, but at a different scale\n",
    "* images are now 28x28\n",
    "* 10 different digits\n",
    "* 50k samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((50000, 1, 28, 28), (50000,))\n"
     ]
    }
   ],
   "source": [
    "from mnist import load_dataset\n",
    "X_train,y_train,X_val,y_val,X_test,y_test = load_dataset()\n",
    "\n",
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_X = T.tensor4(\"X\")\n",
    "\n",
    "#input dimention (None means \"Arbitrary\" and only works at  the first axes [samples])\n",
    "input_shape = [None,1,28,28]\n",
    "\n",
    "target_y = T.vector(\"target Y integer\",dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "l = InputLayer(shape = input_shape,input_var=input_X)\n",
    "l=Conv2DLayer(l,32,5)\n",
    "l=MaxPool2DLayer(l,2)\n",
    "l=DropoutLayer(l,0.5)\n",
    "l = DenseLayer(l,num_units=100)\n",
    "l=DropoutLayer(l,0.5)\n",
    "l_out = DenseLayer(l,num_units = 10,nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network prediction (theano-transformation)\n",
    "y_predicted = lasagne.layers.get_output(l_out)\n",
    "y_predicted_test = lasagne.layers.get_output(l_out,deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, b, W, b, W, b]\n"
     ]
    }
   ],
   "source": [
    "#all network weights (shared variables)\n",
    "all_weights = lasagne.layers.get_all_params(l_out)\n",
    "print all_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Than you could simply\n",
    "* define loss function manually\n",
    "* compute error gradient over all weights\n",
    "* define updates\n",
    "* But that's a whole lot of work and life's short\n",
    "  * not to mention life's too short to wait for SGD to converge\n",
    "\n",
    "Instead, we shall use Lasagne builtins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean categorical crossentropy as a loss function - similar to logistic loss but for multiclass targets\n",
    "loss = lasagne.objectives.categorical_crossentropy(y_predicted,target_y).mean()\n",
    "\n",
    "#prediction accuracy\n",
    "accuracy = lasagne.objectives.categorical_accuracy(y_predicted,target_y).mean()\n",
    "accuracy_test = lasagne.objectives.categorical_accuracy(y_predicted_test,target_y).mean()\n",
    "\n",
    "\n",
    "#This function computes gradient AND composes weight updates just like you did earlier\n",
    "updates_sgd = lasagne.updates.sgd(loss, all_weights,learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function that computes loss and updates weights\n",
    "train_fun = theano.function([input_X,target_y],[loss,accuracy],updates= updates_sgd)\n",
    "\n",
    "#function that just computes accuracy\n",
    "accuracy_fun = theano.function([input_X,target_y],accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's all, now let's train it!\n",
    "* We got a lot of data, so it's recommended that you use SGD\n",
    "* So let's implement a function that splits the training sample into minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An auxilary function that returns mini-batches for neural network training\n",
    "\n",
    "#Parameters\n",
    "# inputs - a tensor of images with shape (many, 1, 28, 28), e.g. X_train\n",
    "# outputs - a vector of answers for corresponding images e.g. Y_train\n",
    "#batch_size - a single number - the intended size of each batches\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize):\n",
    "    assert len(inputs) == len(targets)\n",
    "    indices = np.arange(len(inputs))\n",
    "    np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 150, 0.613min\n",
      "  training loss (in-iteration):\t\t0.701184\n",
      "  train accuracy:\t\t77.45 %\n",
      "  validation accuracy:\t\t94.29 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 2 of 150, 1.225min\n",
      "  training loss (in-iteration):\t\t0.311407\n",
      "  train accuracy:\t\t90.58 %\n",
      "  validation accuracy:\t\t96.19 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 3 of 150, 1.837min\n",
      "  training loss (in-iteration):\t\t0.233136\n",
      "  train accuracy:\t\t93.13 %\n",
      "  validation accuracy:\t\t97.12 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 4 of 150, 2.449min\n",
      "  training loss (in-iteration):\t\t0.194031\n",
      "  train accuracy:\t\t94.24 %\n",
      "  validation accuracy:\t\t97.46 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 5 of 150, 3.081min\n",
      "  training loss (in-iteration):\t\t0.165891\n",
      "  train accuracy:\t\t95.04 %\n",
      "  validation accuracy:\t\t97.85 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 6 of 150, 3.701min\n",
      "  training loss (in-iteration):\t\t0.149173\n",
      "  train accuracy:\t\t95.50 %\n",
      "  validation accuracy:\t\t97.99 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 7 of 150, 4.325min\n",
      "  training loss (in-iteration):\t\t0.136241\n",
      "  train accuracy:\t\t95.87 %\n",
      "  validation accuracy:\t\t98.16 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 8 of 150, 4.943min\n",
      "  training loss (in-iteration):\t\t0.129655\n",
      "  train accuracy:\t\t96.08 %\n",
      "  validation accuracy:\t\t98.39 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 9 of 150, 5.561min\n",
      "  training loss (in-iteration):\t\t0.121129\n",
      "  train accuracy:\t\t96.30 %\n",
      "  validation accuracy:\t\t98.38 %\n",
      "Epoch 10 of 150, 6.183min\n",
      "  training loss (in-iteration):\t\t0.112798\n",
      "  train accuracy:\t\t96.67 %\n",
      "  validation accuracy:\t\t98.48 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 11 of 150, 6.805min\n",
      "  training loss (in-iteration):\t\t0.107356\n",
      "  train accuracy:\t\t96.76 %\n",
      "  validation accuracy:\t\t98.54 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 12 of 150, 7.428min\n",
      "  training loss (in-iteration):\t\t0.099358\n",
      "  train accuracy:\t\t96.94 %\n",
      "  validation accuracy:\t\t98.49 %\n",
      "Epoch 13 of 150, 8.047min\n",
      "  training loss (in-iteration):\t\t0.096394\n",
      "  train accuracy:\t\t97.01 %\n",
      "  validation accuracy:\t\t98.62 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 14 of 150, 8.669min\n",
      "  training loss (in-iteration):\t\t0.095334\n",
      "  train accuracy:\t\t97.09 %\n",
      "  validation accuracy:\t\t98.63 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 15 of 150, 9.287min\n",
      "  training loss (in-iteration):\t\t0.088976\n",
      "  train accuracy:\t\t97.32 %\n",
      "  validation accuracy:\t\t98.57 %\n",
      "Epoch 16 of 150, 9.909min\n",
      "  training loss (in-iteration):\t\t0.086945\n",
      "  train accuracy:\t\t97.31 %\n",
      "  validation accuracy:\t\t98.70 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 17 of 150, 10.529min\n",
      "  training loss (in-iteration):\t\t0.082794\n",
      "  train accuracy:\t\t97.47 %\n",
      "  validation accuracy:\t\t98.68 %\n",
      "Epoch 18 of 150, 11.149min\n",
      "  training loss (in-iteration):\t\t0.079294\n",
      "  train accuracy:\t\t97.63 %\n",
      "  validation accuracy:\t\t98.75 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 19 of 150, 11.769min\n",
      "  training loss (in-iteration):\t\t0.079002\n",
      "  train accuracy:\t\t97.54 %\n",
      "  validation accuracy:\t\t98.67 %\n",
      "Epoch 20 of 150, 12.389min\n",
      "  training loss (in-iteration):\t\t0.075633\n",
      "  train accuracy:\t\t97.68 %\n",
      "  validation accuracy:\t\t98.80 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 21 of 150, 13.012min\n",
      "  training loss (in-iteration):\t\t0.076129\n",
      "  train accuracy:\t\t97.71 %\n",
      "  validation accuracy:\t\t98.77 %\n",
      "Epoch 22 of 150, 13.652min\n",
      "  training loss (in-iteration):\t\t0.072154\n",
      "  train accuracy:\t\t97.78 %\n",
      "  validation accuracy:\t\t98.78 %\n",
      "Epoch 23 of 150, 14.275min\n",
      "  training loss (in-iteration):\t\t0.071178\n",
      "  train accuracy:\t\t97.86 %\n",
      "  validation accuracy:\t\t98.79 %\n",
      "Epoch 24 of 150, 14.897min\n",
      "  training loss (in-iteration):\t\t0.070094\n",
      "  train accuracy:\t\t97.82 %\n",
      "  validation accuracy:\t\t98.81 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 25 of 150, 15.522min\n",
      "  training loss (in-iteration):\t\t0.069161\n",
      "  train accuracy:\t\t97.87 %\n",
      "  validation accuracy:\t\t98.77 %\n",
      "Epoch 26 of 150, 16.146min\n",
      "  training loss (in-iteration):\t\t0.064071\n",
      "  train accuracy:\t\t98.02 %\n",
      "  validation accuracy:\t\t98.85 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 27 of 150, 16.773min\n",
      "  training loss (in-iteration):\t\t0.065824\n",
      "  train accuracy:\t\t98.00 %\n",
      "  validation accuracy:\t\t98.88 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 28 of 150, 17.398min\n",
      "  training loss (in-iteration):\t\t0.065548\n",
      "  train accuracy:\t\t97.95 %\n",
      "  validation accuracy:\t\t98.86 %\n",
      "Epoch 29 of 150, 18.024min\n",
      "  training loss (in-iteration):\t\t0.061991\n",
      "  train accuracy:\t\t98.10 %\n",
      "  validation accuracy:\t\t98.83 %\n",
      "Epoch 30 of 150, 18.647min\n",
      "  training loss (in-iteration):\t\t0.063576\n",
      "  train accuracy:\t\t98.01 %\n",
      "  validation accuracy:\t\t98.89 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 31 of 150, 19.269min\n",
      "  training loss (in-iteration):\t\t0.059932\n",
      "  train accuracy:\t\t98.10 %\n",
      "  validation accuracy:\t\t98.88 %\n",
      "Epoch 32 of 150, 19.892min\n",
      "  training loss (in-iteration):\t\t0.059164\n",
      "  train accuracy:\t\t98.13 %\n",
      "  validation accuracy:\t\t98.91 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 33 of 150, 20.517min\n",
      "  training loss (in-iteration):\t\t0.059710\n",
      "  train accuracy:\t\t98.14 %\n",
      "  validation accuracy:\t\t98.87 %\n",
      "Epoch 34 of 150, 21.145min\n",
      "  training loss (in-iteration):\t\t0.058406\n",
      "  train accuracy:\t\t98.18 %\n",
      "  validation accuracy:\t\t98.87 %\n",
      "Epoch 35 of 150, 21.773min\n",
      "  training loss (in-iteration):\t\t0.056881\n",
      "  train accuracy:\t\t98.19 %\n",
      "  validation accuracy:\t\t98.90 %\n",
      "Epoch 36 of 150, 22.390min\n",
      "  training loss (in-iteration):\t\t0.056554\n",
      "  train accuracy:\t\t98.27 %\n",
      "  validation accuracy:\t\t98.91 %\n",
      "Epoch 37 of 150, 23.008min\n",
      "  training loss (in-iteration):\t\t0.056812\n",
      "  train accuracy:\t\t98.22 %\n",
      "  validation accuracy:\t\t98.89 %\n",
      "Epoch 38 of 150, 23.626min\n",
      "  training loss (in-iteration):\t\t0.054834\n",
      "  train accuracy:\t\t98.28 %\n",
      "  validation accuracy:\t\t98.90 %\n",
      "Epoch 39 of 150, 24.246min\n",
      "  training loss (in-iteration):\t\t0.053973\n",
      "  train accuracy:\t\t98.29 %\n",
      "  validation accuracy:\t\t98.94 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 40 of 150, 24.865min\n",
      "  training loss (in-iteration):\t\t0.054195\n",
      "  train accuracy:\t\t98.35 %\n",
      "  validation accuracy:\t\t98.89 %\n",
      "Epoch 41 of 150, 25.486min\n",
      "  training loss (in-iteration):\t\t0.051334\n",
      "  train accuracy:\t\t98.43 %\n",
      "  validation accuracy:\t\t98.90 %\n",
      "Epoch 42 of 150, 26.112min\n",
      "  training loss (in-iteration):\t\t0.052363\n",
      "  train accuracy:\t\t98.38 %\n",
      "  validation accuracy:\t\t98.94 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 43 of 150, 26.738min\n",
      "  training loss (in-iteration):\t\t0.051232\n",
      "  train accuracy:\t\t98.35 %\n",
      "  validation accuracy:\t\t98.94 %\n",
      "Epoch 44 of 150, 27.362min\n",
      "  training loss (in-iteration):\t\t0.051113\n",
      "  train accuracy:\t\t98.34 %\n",
      "  validation accuracy:\t\t98.99 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 45 of 150, 27.998min\n",
      "  training loss (in-iteration):\t\t0.051436\n",
      "  train accuracy:\t\t98.36 %\n",
      "  validation accuracy:\t\t98.92 %\n",
      "Epoch 46 of 150, 28.621min\n",
      "  training loss (in-iteration):\t\t0.048477\n",
      "  train accuracy:\t\t98.47 %\n",
      "  validation accuracy:\t\t98.97 %\n",
      "Epoch 47 of 150, 29.241min\n",
      "  training loss (in-iteration):\t\t0.048672\n",
      "  train accuracy:\t\t98.50 %\n",
      "  validation accuracy:\t\t98.93 %\n",
      "Epoch 48 of 150, 29.859min\n",
      "  training loss (in-iteration):\t\t0.052532\n",
      "  train accuracy:\t\t98.35 %\n",
      "  validation accuracy:\t\t99.01 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 49 of 150, 30.481min\n",
      "  training loss (in-iteration):\t\t0.046839\n",
      "  train accuracy:\t\t98.52 %\n",
      "  validation accuracy:\t\t98.97 %\n",
      "Epoch 50 of 150, 31.099min\n",
      "  training loss (in-iteration):\t\t0.044037\n",
      "  train accuracy:\t\t98.59 %\n",
      "  validation accuracy:\t\t99.03 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 51 of 150, 31.720min\n",
      "  training loss (in-iteration):\t\t0.048348\n",
      "  train accuracy:\t\t98.46 %\n",
      "  validation accuracy:\t\t99.06 %\n",
      "-------------------------------------------------------------Curent Max\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 of 150, 32.339min\n",
      "  training loss (in-iteration):\t\t0.045247\n",
      "  train accuracy:\t\t98.52 %\n",
      "  validation accuracy:\t\t98.97 %\n",
      "Epoch 53 of 150, 32.959min\n",
      "  training loss (in-iteration):\t\t0.045307\n",
      "  train accuracy:\t\t98.57 %\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 54 of 150, 33.577min\n",
      "  training loss (in-iteration):\t\t0.045091\n",
      "  train accuracy:\t\t98.56 %\n",
      "  validation accuracy:\t\t99.05 %\n",
      "Epoch 55 of 150, 34.194min\n",
      "  training loss (in-iteration):\t\t0.043378\n",
      "  train accuracy:\t\t98.65 %\n",
      "  validation accuracy:\t\t98.98 %\n",
      "Epoch 56 of 150, 34.819min\n",
      "  training loss (in-iteration):\t\t0.044104\n",
      "  train accuracy:\t\t98.57 %\n",
      "  validation accuracy:\t\t98.97 %\n",
      "Epoch 57 of 150, 35.443min\n",
      "  training loss (in-iteration):\t\t0.043728\n",
      "  train accuracy:\t\t98.59 %\n",
      "  validation accuracy:\t\t99.02 %\n",
      "Epoch 58 of 150, 36.067min\n",
      "  training loss (in-iteration):\t\t0.044171\n",
      "  train accuracy:\t\t98.58 %\n",
      "  validation accuracy:\t\t99.02 %\n",
      "Epoch 59 of 150, 36.693min\n",
      "  training loss (in-iteration):\t\t0.041362\n",
      "  train accuracy:\t\t98.64 %\n",
      "  validation accuracy:\t\t99.00 %\n",
      "Epoch 60 of 150, 37.316min\n",
      "  training loss (in-iteration):\t\t0.042096\n",
      "  train accuracy:\t\t98.68 %\n",
      "  validation accuracy:\t\t98.99 %\n",
      "Epoch 61 of 150, 37.946min\n",
      "  training loss (in-iteration):\t\t0.041191\n",
      "  train accuracy:\t\t98.66 %\n",
      "  validation accuracy:\t\t98.92 %\n",
      "Epoch 62 of 150, 38.621min\n",
      "  training loss (in-iteration):\t\t0.042102\n",
      "  train accuracy:\t\t98.67 %\n",
      "  validation accuracy:\t\t99.00 %\n",
      "Epoch 63 of 150, 39.267min\n",
      "  training loss (in-iteration):\t\t0.039976\n",
      "  train accuracy:\t\t98.70 %\n",
      "  validation accuracy:\t\t98.99 %\n",
      "Epoch 64 of 150, 39.907min\n",
      "  training loss (in-iteration):\t\t0.040257\n",
      "  train accuracy:\t\t98.72 %\n",
      "  validation accuracy:\t\t99.01 %\n",
      "Epoch 65 of 150, 40.550min\n",
      "  training loss (in-iteration):\t\t0.041414\n",
      "  train accuracy:\t\t98.68 %\n",
      "  validation accuracy:\t\t98.98 %\n",
      "Epoch 66 of 150, 41.205min\n",
      "  training loss (in-iteration):\t\t0.041742\n",
      "  train accuracy:\t\t98.68 %\n",
      "  validation accuracy:\t\t98.99 %\n",
      "Epoch 67 of 150, 41.842min\n",
      "  training loss (in-iteration):\t\t0.038511\n",
      "  train accuracy:\t\t98.77 %\n",
      "  validation accuracy:\t\t99.01 %\n",
      "Epoch 68 of 150, 42.473min\n",
      "  training loss (in-iteration):\t\t0.039315\n",
      "  train accuracy:\t\t98.73 %\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 69 of 150, 43.100min\n",
      "  training loss (in-iteration):\t\t0.040325\n",
      "  train accuracy:\t\t98.69 %\n",
      "  validation accuracy:\t\t98.99 %\n",
      "Epoch 70 of 150, 43.727min\n",
      "  training loss (in-iteration):\t\t0.038190\n",
      "  train accuracy:\t\t98.73 %\n",
      "  validation accuracy:\t\t98.97 %\n",
      "Epoch 71 of 150, 44.348min\n",
      "  training loss (in-iteration):\t\t0.039727\n",
      "  train accuracy:\t\t98.71 %\n",
      "  validation accuracy:\t\t98.99 %\n",
      "Epoch 72 of 150, 44.975min\n",
      "  training loss (in-iteration):\t\t0.037034\n",
      "  train accuracy:\t\t98.79 %\n",
      "  validation accuracy:\t\t99.01 %\n",
      "Epoch 73 of 150, 45.594min\n",
      "  training loss (in-iteration):\t\t0.039048\n",
      "  train accuracy:\t\t98.69 %\n",
      "  validation accuracy:\t\t98.99 %\n",
      "Epoch 74 of 150, 46.217min\n",
      "  training loss (in-iteration):\t\t0.038691\n",
      "  train accuracy:\t\t98.72 %\n",
      "  validation accuracy:\t\t98.99 %\n",
      "Epoch 75 of 150, 46.888min\n",
      "  training loss (in-iteration):\t\t0.037291\n",
      "  train accuracy:\t\t98.82 %\n",
      "  validation accuracy:\t\t99.02 %\n",
      "Epoch 76 of 150, 47.505min\n",
      "  training loss (in-iteration):\t\t0.036996\n",
      "  train accuracy:\t\t98.77 %\n",
      "  validation accuracy:\t\t99.00 %\n",
      "Epoch 77 of 150, 48.126min\n",
      "  training loss (in-iteration):\t\t0.036310\n",
      "  train accuracy:\t\t98.77 %\n",
      "  validation accuracy:\t\t99.01 %\n",
      "Epoch 78 of 150, 48.752min\n",
      "  training loss (in-iteration):\t\t0.036632\n",
      "  train accuracy:\t\t98.85 %\n",
      "  validation accuracy:\t\t98.99 %\n",
      "Epoch 79 of 150, 49.439min\n",
      "  training loss (in-iteration):\t\t0.037056\n",
      "  train accuracy:\t\t98.80 %\n",
      "  validation accuracy:\t\t98.99 %\n",
      "Epoch 80 of 150, 50.069min\n",
      "  training loss (in-iteration):\t\t0.036352\n",
      "  train accuracy:\t\t98.82 %\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 81 of 150, 50.688min\n",
      "  training loss (in-iteration):\t\t0.036149\n",
      "  train accuracy:\t\t98.80 %\n",
      "  validation accuracy:\t\t99.05 %\n",
      "Epoch 82 of 150, 51.307min\n",
      "  training loss (in-iteration):\t\t0.035626\n",
      "  train accuracy:\t\t98.82 %\n",
      "  validation accuracy:\t\t99.07 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 83 of 150, 51.930min\n",
      "  training loss (in-iteration):\t\t0.034820\n",
      "  train accuracy:\t\t98.88 %\n",
      "  validation accuracy:\t\t98.98 %\n",
      "Epoch 84 of 150, 52.557min\n",
      "  training loss (in-iteration):\t\t0.033007\n",
      "  train accuracy:\t\t98.91 %\n",
      "  validation accuracy:\t\t99.01 %\n",
      "Epoch 85 of 150, 53.173min\n",
      "  training loss (in-iteration):\t\t0.034073\n",
      "  train accuracy:\t\t98.88 %\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 86 of 150, 53.790min\n",
      "  training loss (in-iteration):\t\t0.032624\n",
      "  train accuracy:\t\t98.90 %\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 87 of 150, 54.405min\n",
      "  training loss (in-iteration):\t\t0.034425\n",
      "  train accuracy:\t\t98.90 %\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 88 of 150, 55.025min\n",
      "  training loss (in-iteration):\t\t0.034599\n",
      "  train accuracy:\t\t98.84 %\n",
      "  validation accuracy:\t\t98.99 %\n",
      "Epoch 89 of 150, 55.646min\n",
      "  training loss (in-iteration):\t\t0.035308\n",
      "  train accuracy:\t\t98.87 %\n",
      "  validation accuracy:\t\t99.01 %\n",
      "Epoch 90 of 150, 56.264min\n",
      "  training loss (in-iteration):\t\t0.033122\n",
      "  train accuracy:\t\t98.94 %\n",
      "  validation accuracy:\t\t99.10 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 91 of 150, 56.884min\n",
      "  training loss (in-iteration):\t\t0.031752\n",
      "  train accuracy:\t\t98.93 %\n",
      "  validation accuracy:\t\t99.12 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 92 of 150, 57.505min\n",
      "  training loss (in-iteration):\t\t0.032955\n",
      "  train accuracy:\t\t98.90 %\n",
      "  validation accuracy:\t\t99.10 %\n",
      "Epoch 93 of 150, 58.119min\n",
      "  training loss (in-iteration):\t\t0.033056\n",
      "  train accuracy:\t\t98.90 %\n",
      "  validation accuracy:\t\t99.09 %\n",
      "Epoch 94 of 150, 58.737min\n",
      "  training loss (in-iteration):\t\t0.032506\n",
      "  train accuracy:\t\t98.93 %\n",
      "  validation accuracy:\t\t99.06 %\n",
      "Epoch 95 of 150, 59.352min\n",
      "  training loss (in-iteration):\t\t0.032656\n",
      "  train accuracy:\t\t98.91 %\n",
      "  validation accuracy:\t\t99.05 %\n",
      "Epoch 96 of 150, 59.963min\n",
      "  training loss (in-iteration):\t\t0.032171\n",
      "  train accuracy:\t\t98.95 %\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 97 of 150, 60.576min\n",
      "  training loss (in-iteration):\t\t0.030917\n",
      "  train accuracy:\t\t99.00 %\n",
      "  validation accuracy:\t\t99.05 %\n",
      "Epoch 98 of 150, 61.187min\n",
      "  training loss (in-iteration):\t\t0.031567\n",
      "  train accuracy:\t\t98.98 %\n",
      "  validation accuracy:\t\t99.08 %\n",
      "Epoch 99 of 150, 61.800min\n",
      "  training loss (in-iteration):\t\t0.030821\n",
      "  train accuracy:\t\t99.01 %\n",
      "  validation accuracy:\t\t99.06 %\n",
      "Epoch 100 of 150, 62.415min\n",
      "  training loss (in-iteration):\t\t0.032561\n",
      "  train accuracy:\t\t98.95 %\n",
      "  validation accuracy:\t\t99.06 %\n",
      "Epoch 101 of 150, 63.040min\n",
      "  training loss (in-iteration):\t\t0.031524\n",
      "  train accuracy:\t\t98.95 %\n",
      "  validation accuracy:\t\t99.11 %\n",
      "Epoch 102 of 150, 63.659min\n",
      "  training loss (in-iteration):\t\t0.032362\n",
      "  train accuracy:\t\t98.93 %\n",
      "  validation accuracy:\t\t99.09 %\n",
      "Epoch 103 of 150, 64.273min\n",
      "  training loss (in-iteration):\t\t0.030806\n",
      "  train accuracy:\t\t98.98 %\n",
      "  validation accuracy:\t\t99.07 %\n",
      "Epoch 104 of 150, 64.885min\n",
      "  training loss (in-iteration):\t\t0.031992\n",
      "  train accuracy:\t\t98.93 %\n",
      "  validation accuracy:\t\t99.01 %\n",
      "Epoch 105 of 150, 65.496min\n",
      "  training loss (in-iteration):\t\t0.030306\n",
      "  train accuracy:\t\t99.00 %\n",
      "  validation accuracy:\t\t99.08 %\n",
      "Epoch 106 of 150, 66.107min\n",
      "  training loss (in-iteration):\t\t0.030402\n",
      "  train accuracy:\t\t98.99 %\n",
      "  validation accuracy:\t\t99.07 %\n",
      "Epoch 107 of 150, 66.719min\n",
      "  training loss (in-iteration):\t\t0.030644\n",
      "  train accuracy:\t\t99.00 %\n",
      "  validation accuracy:\t\t99.10 %\n",
      "Epoch 108 of 150, 67.334min\n",
      "  training loss (in-iteration):\t\t0.031247\n",
      "  train accuracy:\t\t98.96 %\n",
      "  validation accuracy:\t\t99.01 %\n",
      "Epoch 109 of 150, 67.947min\n",
      "  training loss (in-iteration):\t\t0.030546\n",
      "  train accuracy:\t\t98.99 %\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 110 of 150, 68.571min\n",
      "  training loss (in-iteration):\t\t0.029659\n",
      "  train accuracy:\t\t98.98 %\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 111 of 150, 69.183min\n",
      "  training loss (in-iteration):\t\t0.028964\n",
      "  train accuracy:\t\t99.07 %\n",
      "  validation accuracy:\t\t99.07 %\n",
      "Epoch 112 of 150, 69.803min\n",
      "  training loss (in-iteration):\t\t0.028777\n",
      "  train accuracy:\t\t99.04 %\n",
      "  validation accuracy:\t\t99.07 %\n",
      "Epoch 113 of 150, 70.418min\n",
      "  training loss (in-iteration):\t\t0.029571\n",
      "  train accuracy:\t\t99.00 %\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 114 of 150, 71.032min\n",
      "  training loss (in-iteration):\t\t0.026782\n",
      "  train accuracy:\t\t99.12 %\n",
      "  validation accuracy:\t\t99.09 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115 of 150, 71.646min\n",
      "  training loss (in-iteration):\t\t0.029404\n",
      "  train accuracy:\t\t99.00 %\n",
      "  validation accuracy:\t\t99.13 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 116 of 150, 72.259min\n",
      "  training loss (in-iteration):\t\t0.028881\n",
      "  train accuracy:\t\t99.02 %\n",
      "  validation accuracy:\t\t99.08 %\n",
      "Epoch 117 of 150, 72.875min\n",
      "  training loss (in-iteration):\t\t0.030024\n",
      "  train accuracy:\t\t98.95 %\n",
      "  validation accuracy:\t\t99.06 %\n",
      "Epoch 118 of 150, 73.488min\n",
      "  training loss (in-iteration):\t\t0.030034\n",
      "  train accuracy:\t\t99.00 %\n",
      "  validation accuracy:\t\t99.09 %\n",
      "Epoch 119 of 150, 74.102min\n",
      "  training loss (in-iteration):\t\t0.028734\n",
      "  train accuracy:\t\t99.04 %\n",
      "  validation accuracy:\t\t99.11 %\n",
      "Epoch 120 of 150, 74.717min\n",
      "  training loss (in-iteration):\t\t0.027704\n",
      "  train accuracy:\t\t99.06 %\n",
      "  validation accuracy:\t\t99.10 %\n",
      "Epoch 121 of 150, 75.331min\n",
      "  training loss (in-iteration):\t\t0.027533\n",
      "  train accuracy:\t\t99.10 %\n",
      "  validation accuracy:\t\t99.07 %\n",
      "Epoch 122 of 150, 75.947min\n",
      "  training loss (in-iteration):\t\t0.028924\n",
      "  train accuracy:\t\t99.08 %\n",
      "  validation accuracy:\t\t99.07 %\n",
      "Epoch 123 of 150, 76.562min\n",
      "  training loss (in-iteration):\t\t0.028328\n",
      "  train accuracy:\t\t99.09 %\n",
      "  validation accuracy:\t\t99.08 %\n",
      "Epoch 124 of 150, 77.178min\n",
      "  training loss (in-iteration):\t\t0.027589\n",
      "  train accuracy:\t\t99.08 %\n",
      "  validation accuracy:\t\t99.06 %\n",
      "Epoch 125 of 150, 77.794min\n",
      "  training loss (in-iteration):\t\t0.027114\n",
      "  train accuracy:\t\t99.08 %\n",
      "  validation accuracy:\t\t99.12 %\n",
      "Epoch 126 of 150, 78.404min\n",
      "  training loss (in-iteration):\t\t0.027786\n",
      "  train accuracy:\t\t99.07 %\n",
      "  validation accuracy:\t\t99.09 %\n",
      "Epoch 127 of 150, 79.020min\n",
      "  training loss (in-iteration):\t\t0.028697\n",
      "  train accuracy:\t\t98.99 %\n",
      "  validation accuracy:\t\t99.03 %\n",
      "Epoch 128 of 150, 79.631min\n",
      "  training loss (in-iteration):\t\t0.027287\n",
      "  train accuracy:\t\t99.07 %\n",
      "  validation accuracy:\t\t99.11 %\n",
      "Epoch 129 of 150, 80.258min\n",
      "  training loss (in-iteration):\t\t0.027728\n",
      "  train accuracy:\t\t99.11 %\n",
      "  validation accuracy:\t\t99.05 %\n",
      "Epoch 130 of 150, 80.876min\n",
      "  training loss (in-iteration):\t\t0.026782\n",
      "  train accuracy:\t\t99.10 %\n",
      "  validation accuracy:\t\t99.12 %\n",
      "Epoch 131 of 150, 81.491min\n",
      "  training loss (in-iteration):\t\t0.026855\n",
      "  train accuracy:\t\t99.09 %\n",
      "  validation accuracy:\t\t99.11 %\n",
      "Epoch 132 of 150, 82.111min\n",
      "  training loss (in-iteration):\t\t0.026377\n",
      "  train accuracy:\t\t99.08 %\n",
      "  validation accuracy:\t\t99.14 %\n",
      "-------------------------------------------------------------Curent Max\n",
      "Epoch 133 of 150, 82.733min\n",
      "  training loss (in-iteration):\t\t0.027483\n",
      "  train accuracy:\t\t99.05 %\n",
      "  validation accuracy:\t\t99.11 %\n",
      "Epoch 134 of 150, 83.349min\n",
      "  training loss (in-iteration):\t\t0.026344\n",
      "  train accuracy:\t\t99.11 %\n",
      "  validation accuracy:\t\t99.08 %\n",
      "Epoch 135 of 150, 83.967min\n",
      "  training loss (in-iteration):\t\t0.026115\n",
      "  train accuracy:\t\t99.12 %\n",
      "  validation accuracy:\t\t99.07 %\n",
      "Epoch 136 of 150, 84.590min\n",
      "  training loss (in-iteration):\t\t0.025930\n",
      "  train accuracy:\t\t99.14 %\n",
      "  validation accuracy:\t\t99.08 %\n",
      "Epoch 137 of 150, 85.204min\n",
      "  training loss (in-iteration):\t\t0.024557\n",
      "  train accuracy:\t\t99.15 %\n",
      "  validation accuracy:\t\t99.04 %\n",
      "Epoch 138 of 150, 85.821min\n",
      "  training loss (in-iteration):\t\t0.024633\n",
      "  train accuracy:\t\t99.13 %\n",
      "  validation accuracy:\t\t99.06 %\n",
      "Epoch 139 of 150, 86.436min\n",
      "  training loss (in-iteration):\t\t0.025155\n",
      "  train accuracy:\t\t99.16 %\n",
      "  validation accuracy:\t\t99.05 %\n",
      "Epoch 140 of 150, 87.050min\n",
      "  training loss (in-iteration):\t\t0.024321\n",
      "  train accuracy:\t\t99.18 %\n",
      "  validation accuracy:\t\t99.05 %\n",
      "Epoch 141 of 150, 87.668min\n",
      "  training loss (in-iteration):\t\t0.026118\n",
      "  train accuracy:\t\t99.12 %\n",
      "  validation accuracy:\t\t99.06 %\n",
      "Epoch 142 of 150, 88.285min\n",
      "  training loss (in-iteration):\t\t0.026043\n",
      "  train accuracy:\t\t99.11 %\n",
      "  validation accuracy:\t\t99.08 %\n",
      "Epoch 143 of 150, 88.903min\n",
      "  training loss (in-iteration):\t\t0.025406\n",
      "  train accuracy:\t\t99.10 %\n",
      "  validation accuracy:\t\t99.13 %\n",
      "Epoch 144 of 150, 89.517min\n",
      "  training loss (in-iteration):\t\t0.025372\n",
      "  train accuracy:\t\t99.15 %\n",
      "  validation accuracy:\t\t99.08 %\n",
      "Epoch 145 of 150, 90.135min\n",
      "  training loss (in-iteration):\t\t0.023106\n",
      "  train accuracy:\t\t99.23 %\n",
      "  validation accuracy:\t\t99.07 %\n",
      "Epoch 146 of 150, 90.749min\n",
      "  training loss (in-iteration):\t\t0.025373\n",
      "  train accuracy:\t\t99.16 %\n",
      "  validation accuracy:\t\t99.13 %\n",
      "Epoch 147 of 150, 91.363min\n",
      "  training loss (in-iteration):\t\t0.025296\n",
      "  train accuracy:\t\t99.16 %\n",
      "  validation accuracy:\t\t99.08 %\n",
      "Epoch 148 of 150, 91.976min\n",
      "  training loss (in-iteration):\t\t0.024278\n",
      "  train accuracy:\t\t99.17 %\n",
      "  validation accuracy:\t\t99.10 %\n",
      "Epoch 149 of 150, 92.590min\n",
      "  training loss (in-iteration):\t\t0.023853\n",
      "  train accuracy:\t\t99.23 %\n",
      "  validation accuracy:\t\t99.08 %\n",
      "Epoch 150 of 150, 93.208min\n",
      "  training loss (in-iteration):\t\t0.023989\n",
      "  train accuracy:\t\t99.19 %\n",
      "  validation accuracy:\t\t99.06 %\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 150 #amount of passes through the data\n",
    "\n",
    "batch_size = 200 #number of samples processed at each function call\n",
    "\n",
    "loss_hist=[]\n",
    "\n",
    "start_time = time.time()\n",
    "max_accur=0\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    for batch in iterate_minibatches(X_train, y_train,batch_size):\n",
    "        inputs, targets = batch\n",
    "        train_err_batch, train_acc_batch= train_fun(inputs, targets)\n",
    "        train_err += train_err_batch\n",
    "        train_acc += train_acc_batch\n",
    "        train_batches += 1\n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "        inputs, targets = batch\n",
    "        val_acc += accuracy_fun(inputs, targets)\n",
    "        val_batches += 1\n",
    "\n",
    "    \n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {}, {:.3f}min\".format(\n",
    "        epoch + 1, num_epochs, (time.time() - start_time)/60))\n",
    "    \n",
    "    loss_hist.append(train_err / train_batches)\n",
    "    print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "        train_acc / train_batches * 100))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))\n",
    "    if val_acc>max_accur:\n",
    "        max_accur=val_acc\n",
    "        print(\"-------------------------------------------------------------Curent Max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t99.20 %\n",
      "Achievement unlocked: 80lvl Warlock!\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500):\n",
    "    inputs, targets = batch\n",
    "    acc = accuracy_fun(inputs, targets)\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_acc / test_batches * 100))\n",
    "\n",
    "if test_acc / test_batches * 100 > 99:\n",
    "    print (\"Achievement unlocked: 80lvl Warlock!\")\n",
    "else:\n",
    "    print (\"We need more magic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Now improve it!\n",
    "\n",
    "* Moar layers!\n",
    "* Moar units!\n",
    "* Different nonlinearities!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "I used the follow NN:\n",
    "\n",
    "1. Conv(32,5)\n",
    "2. MaxPool(2)\n",
    "5. Dropout(0.5)\n",
    "6. Dense(100)\n",
    "7. Dropout(0.5)\n",
    "8. Dense(10)\n",
    "\n",
    "As we can see, the stable 99% accuracy on validation we get on 55 min.\n",
    "\n",
    "Device parameters: i7-4770 CPU, 3.40×8, 16 RAM\n",
    "\n",
    "#### I've learned the following:\n",
    "\n",
    "1. Dropout laters prevent overfiting. But one should don't forget to remove Dropouts by \"deterministic=True\" when test NN\n",
    "2. MaxPool not only save our time but also affect to accuracy. As I understand, after MaxPool, the area which goes to Conv filter becоmes more informative. But one should don't increase pooling level much to avoid information loosing.\n",
    "3. Linear is faster than other nonlinearities but nonlinearities better guarantee the convergance.\n",
    "\n",
    "Below you see the history of Loss Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nv7KaPJcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f22060ec710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(num_epochs), loss_hist)\n",
    "plt.title(\"Loss Function history\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
